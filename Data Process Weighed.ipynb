{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8365b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee76e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371  # Radius of the earth in kilometers\n",
    "    phi1 = np.radians(lat1)\n",
    "    phi2 = np.radians(lat2)\n",
    "    delta_phi = np.radians(lat2 - lat1)\n",
    "    delta_lambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(delta_phi/2)**2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda/2)**2\n",
    "    res = r * (2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)))\n",
    "    return np.round(res, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b6a4a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(r'C:\\Users\\24707\\Downloads\\202206-divvy-tripdata\\202206-divvy-tripdata.csv')\n",
    "\n",
    "# Remove rows with 'casual' in the 'member_casual' column\n",
    "\n",
    "df = df[df['member_casual'] != 'casual']\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ed8194",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# Adding edges along with weights\n",
    "for i in range(len(df)):\n",
    "    row_data = df.iloc[i]\n",
    "    lat1, lon1 = row_data['start_lat'], row_data['start_lng']\n",
    "    lat2, lon2 = row_data['end_lat'], row_data['end_lng']\n",
    "    weight = haversine_distance(lat1, lon1, lat2, lon2)\n",
    "    G.add_edge(row_data['start_station_id'], row_data['end_station_id'], weight=weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245fc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of node names to unique integers\n",
    "node_mapping = {node: i for i, node in enumerate(G.nodes)}\n",
    "\n",
    "# Now use this mapping when creating edge_index and edge_weight\n",
    "edge_index = []\n",
    "edge_weight = []\n",
    "for edge in G.edges.data():\n",
    "    # Get the nodes and weight\n",
    "    node1, node2, data = edge\n",
    "    # Map nodes to integers\n",
    "    node1 = node_mapping[node1]\n",
    "    node2 = node_mapping[node2]\n",
    "    edge_index.append((node1, node2))\n",
    "    edge_weight.append(data['weight'])\n",
    "\n",
    "# Convert to tensors\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(edge_weight, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccc9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv, ChebConv  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b02043dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import ChebConv\n",
    "\n",
    "class STGCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.conv1 = ChebConv(num_node_features, hidden_channels, K=2)\n",
    "        self.conv2 = ChebConv(hidden_channels, num_classes, K=2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data['x'], data['edge_index'], data['edge_weight']\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "108051e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SOFTWARES\\Annoconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([693])) that is different to the input size (torch.Size([693, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.4084224700927734\n",
      "Epoch: 2, Loss: 1.9836935997009277\n",
      "Epoch: 3, Loss: 1.895984411239624\n",
      "Epoch: 4, Loss: 1.6935316324234009\n",
      "Epoch: 5, Loss: 1.621172308921814\n",
      "Epoch: 6, Loss: 1.6832197904586792\n",
      "Epoch: 7, Loss: 1.614659309387207\n",
      "Epoch: 8, Loss: 1.524267315864563\n",
      "Epoch: 9, Loss: 1.3947986364364624\n",
      "Epoch: 10, Loss: 1.3775975704193115\n",
      "Epoch: 11, Loss: 1.3041738271713257\n",
      "Epoch: 12, Loss: 1.2688615322113037\n",
      "Epoch: 13, Loss: 1.2115181684494019\n",
      "Epoch: 14, Loss: 1.2319945096969604\n",
      "Epoch: 15, Loss: 1.2184947729110718\n",
      "Epoch: 16, Loss: 1.1592190265655518\n",
      "Epoch: 17, Loss: 1.136747121810913\n",
      "Epoch: 18, Loss: 1.1550792455673218\n",
      "Epoch: 19, Loss: 1.1335463523864746\n",
      "Epoch: 20, Loss: 1.1425673961639404\n",
      "Epoch: 21, Loss: 1.0981675386428833\n",
      "Epoch: 22, Loss: 1.1009050607681274\n",
      "Epoch: 23, Loss: 1.0841503143310547\n",
      "Epoch: 24, Loss: 1.0763030052185059\n",
      "Epoch: 25, Loss: 1.070788025856018\n",
      "Epoch: 26, Loss: 1.049621343612671\n",
      "Epoch: 27, Loss: 1.0383409261703491\n",
      "Epoch: 28, Loss: 1.0363706350326538\n",
      "Epoch: 29, Loss: 1.0265142917633057\n",
      "Epoch: 30, Loss: 1.035006046295166\n",
      "Epoch: 31, Loss: 1.0426498651504517\n",
      "Epoch: 32, Loss: 1.0323047637939453\n",
      "Epoch: 33, Loss: 1.0151978731155396\n",
      "Epoch: 34, Loss: 1.0314253568649292\n",
      "Epoch: 35, Loss: 1.005470633506775\n",
      "Epoch: 36, Loss: 0.9995821714401245\n",
      "Epoch: 37, Loss: 1.00041925907135\n",
      "Epoch: 38, Loss: 0.9995741844177246\n",
      "Epoch: 39, Loss: 1.0051186084747314\n",
      "Epoch: 40, Loss: 0.9886558651924133\n",
      "Epoch: 41, Loss: 0.9993709921836853\n",
      "Epoch: 42, Loss: 0.9941798448562622\n",
      "Epoch: 43, Loss: 0.9940216541290283\n",
      "Epoch: 44, Loss: 0.9893118143081665\n",
      "Epoch: 45, Loss: 0.9865082502365112\n",
      "Epoch: 46, Loss: 0.9887537360191345\n",
      "Epoch: 47, Loss: 0.983717679977417\n",
      "Epoch: 48, Loss: 0.9845099449157715\n",
      "Epoch: 49, Loss: 0.983643114566803\n",
      "Epoch: 50, Loss: 0.9866331219673157\n",
      "Epoch: 51, Loss: 0.9835624098777771\n",
      "Epoch: 52, Loss: 0.9828578233718872\n",
      "Epoch: 53, Loss: 0.9795857071876526\n",
      "Epoch: 54, Loss: 0.9771724939346313\n",
      "Epoch: 55, Loss: 0.9822047352790833\n",
      "Epoch: 56, Loss: 0.9772478938102722\n",
      "Epoch: 57, Loss: 0.9759872555732727\n",
      "Epoch: 58, Loss: 0.982333779335022\n",
      "Epoch: 59, Loss: 0.9755188822746277\n",
      "Epoch: 60, Loss: 0.977644681930542\n",
      "Epoch: 61, Loss: 0.972450315952301\n",
      "Epoch: 62, Loss: 0.9752469062805176\n",
      "Epoch: 63, Loss: 0.9726532697677612\n",
      "Epoch: 64, Loss: 0.9750574827194214\n",
      "Epoch: 65, Loss: 0.9763877987861633\n",
      "Epoch: 66, Loss: 0.9754928946495056\n",
      "Epoch: 67, Loss: 0.9727743268013\n",
      "Epoch: 68, Loss: 0.9745434522628784\n",
      "Epoch: 69, Loss: 0.9701341986656189\n",
      "Epoch: 70, Loss: 0.9708699584007263\n",
      "Epoch: 71, Loss: 0.9686412811279297\n",
      "Epoch: 72, Loss: 0.9708040356636047\n",
      "Epoch: 73, Loss: 0.9680113196372986\n",
      "Epoch: 74, Loss: 0.972978413105011\n",
      "Epoch: 75, Loss: 0.9700151085853577\n",
      "Epoch: 76, Loss: 0.9737820625305176\n",
      "Epoch: 77, Loss: 0.9689338207244873\n",
      "Epoch: 78, Loss: 0.9706779718399048\n",
      "Epoch: 79, Loss: 0.9681426882743835\n",
      "Epoch: 80, Loss: 0.9682469367980957\n",
      "Epoch: 81, Loss: 0.9677019119262695\n",
      "Epoch: 82, Loss: 0.9696389436721802\n",
      "Epoch: 83, Loss: 0.9684826731681824\n",
      "Epoch: 84, Loss: 0.9665642380714417\n",
      "Epoch: 85, Loss: 0.9659426808357239\n",
      "Epoch: 86, Loss: 0.9666538834571838\n",
      "Epoch: 87, Loss: 0.9678666591644287\n",
      "Epoch: 88, Loss: 0.9676867127418518\n",
      "Epoch: 89, Loss: 0.966600239276886\n",
      "Epoch: 90, Loss: 0.9652326703071594\n",
      "Epoch: 91, Loss: 0.9655531644821167\n",
      "Epoch: 92, Loss: 0.9658136963844299\n",
      "Epoch: 93, Loss: 0.9637550711631775\n",
      "Epoch: 94, Loss: 0.9657095074653625\n",
      "Epoch: 95, Loss: 0.9648354053497314\n",
      "Epoch: 96, Loss: 0.9687316417694092\n",
      "Epoch: 97, Loss: 0.9647702574729919\n",
      "Epoch: 98, Loss: 0.9656375050544739\n",
      "Epoch: 99, Loss: 0.9641152024269104\n",
      "Epoch: 100, Loss: 0.9640588164329529\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor for node features (in this case, we generate random features)\n",
    "num_nodes = len(G.nodes())\n",
    "x = torch.randn((num_nodes, 10), dtype=torch.float)\n",
    "\n",
    "# Create a tensor for target values (in this case, we generate random targets)\n",
    "y = torch.randn(num_nodes, dtype=torch.float)\n",
    "\n",
    "# Create the data dictionary\n",
    "data = {\n",
    "    'x': x,\n",
    "    'edge_index': edge_index,\n",
    "    'edge_weight': edge_weight,\n",
    "    'y': y,\n",
    "    'train_mask': train_mask,\n",
    "}\n",
    "\n",
    "# Define the model\n",
    "model = STGCN(num_node_features=10, hidden_channels=16, num_classes=1)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Define the loss function\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "# Start training\n",
    "losses = []\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(data)  # Perform forward pass\n",
    "    loss = loss_func(out[data['train_mask']], data['y'][data['train_mask']])  # Compute the loss\n",
    "    loss.backward()  # Perform backward pass\n",
    "    optimizer.step()  # Update model weights\n",
    "\n",
    "    losses.append(loss.item())  # Store the loss from this epoch\n",
    "    print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4760a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SOFTWARES\\Annoconda\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([148])) that is different to the input size (torch.Size([148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\24707\\AppData\\Local\\Temp\\ipykernel_33560\\1320359241.py:34: UserWarning: Using a target size (torch.Size([148])) that is different to the input size (torch.Size([148, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  val_mae = F.l1_loss(val_out, data['y'][data['val_mask']])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.9659696817398071, Validation Loss: 1.028355360031128, Validation MAE: 0.8119622468948364\n",
      "Epoch: 2, Training Loss: 0.9640079736709595, Validation Loss: 0.9815146923065186, Validation MAE: 0.7937741875648499\n",
      "Epoch: 3, Training Loss: 0.964176595211029, Validation Loss: 0.9778414964675903, Validation MAE: 0.793232798576355\n",
      "Epoch: 4, Training Loss: 0.9652591943740845, Validation Loss: 0.975040853023529, Validation MAE: 0.791785478591919\n",
      "Epoch: 5, Training Loss: 0.9659022092819214, Validation Loss: 1.0356346368789673, Validation MAE: 0.81059330701828\n",
      "Epoch: 6, Training Loss: 0.9656134247779846, Validation Loss: 0.967863917350769, Validation MAE: 0.7901021838188171\n",
      "Epoch: 7, Training Loss: 0.9636872410774231, Validation Loss: 0.9732804894447327, Validation MAE: 0.7918275594711304\n",
      "Epoch: 8, Training Loss: 0.9636361598968506, Validation Loss: 1.050524115562439, Validation MAE: 0.8045141696929932\n",
      "Epoch: 9, Training Loss: 0.9621676802635193, Validation Loss: 0.9803709387779236, Validation MAE: 0.7916225790977478\n",
      "Epoch: 10, Training Loss: 0.9639022350311279, Validation Loss: 0.9811491966247559, Validation MAE: 0.7939310669898987\n",
      "Epoch: 11, Training Loss: 0.9633131623268127, Validation Loss: 0.9802072048187256, Validation MAE: 0.7936708331108093\n",
      "Epoch: 12, Training Loss: 0.962901771068573, Validation Loss: 0.9786468744277954, Validation MAE: 0.793449878692627\n",
      "Epoch: 13, Training Loss: 0.9639763236045837, Validation Loss: 0.971981942653656, Validation MAE: 0.7904342412948608\n",
      "Epoch: 14, Training Loss: 0.9643844366073608, Validation Loss: 0.997581958770752, Validation MAE: 0.7973182797431946\n",
      "Epoch: 15, Training Loss: 0.9638769030570984, Validation Loss: 1.0021482706069946, Validation MAE: 0.7989408373832703\n",
      "Epoch: 16, Training Loss: 0.9640430808067322, Validation Loss: 0.9963722825050354, Validation MAE: 0.8001270890235901\n",
      "Epoch: 17, Training Loss: 0.9638018608093262, Validation Loss: 1.0155625343322754, Validation MAE: 0.8017435669898987\n",
      "Epoch: 18, Training Loss: 0.9646212458610535, Validation Loss: 0.9712437391281128, Validation MAE: 0.7918400764465332\n",
      "Epoch: 19, Training Loss: 0.9640668630599976, Validation Loss: 0.9727755784988403, Validation MAE: 0.7927321791648865\n",
      "Epoch: 20, Training Loss: 0.9630705714225769, Validation Loss: 0.9609267711639404, Validation MAE: 0.7865833044052124\n",
      "Epoch: 21, Training Loss: 0.96226966381073, Validation Loss: 1.120397925376892, Validation MAE: 0.8145354390144348\n",
      "Epoch: 22, Training Loss: 0.9630555510520935, Validation Loss: 0.9744682908058167, Validation MAE: 0.7919248938560486\n",
      "Epoch: 23, Training Loss: 0.962964653968811, Validation Loss: 0.9757072925567627, Validation MAE: 0.7924389243125916\n",
      "Epoch: 24, Training Loss: 0.9630873799324036, Validation Loss: 0.9907881021499634, Validation MAE: 0.7984455823898315\n",
      "Epoch: 25, Training Loss: 0.9627063274383545, Validation Loss: 0.9677106738090515, Validation MAE: 0.7897622585296631\n",
      "Epoch: 26, Training Loss: 0.9626008868217468, Validation Loss: 0.9546294212341309, Validation MAE: 0.7842965126037598\n",
      "Epoch: 27, Training Loss: 0.9621585011482239, Validation Loss: 0.9481531381607056, Validation MAE: 0.782352089881897\n",
      "Epoch: 28, Training Loss: 0.9625192880630493, Validation Loss: 0.9658614993095398, Validation MAE: 0.7887512445449829\n",
      "Epoch: 29, Training Loss: 0.9615148901939392, Validation Loss: 0.9671427607536316, Validation MAE: 0.7887769937515259\n",
      "Epoch: 30, Training Loss: 0.9617681503295898, Validation Loss: 0.9601984620094299, Validation MAE: 0.7862688899040222\n",
      "Epoch: 31, Training Loss: 0.9621054530143738, Validation Loss: 0.9682427048683167, Validation MAE: 0.7884017825126648\n",
      "Epoch: 32, Training Loss: 0.9619702696800232, Validation Loss: 0.9850698113441467, Validation MAE: 0.7950440049171448\n",
      "Epoch: 33, Training Loss: 0.96113520860672, Validation Loss: 0.9655675292015076, Validation MAE: 0.7878453731536865\n",
      "Epoch: 34, Training Loss: 0.9617746472358704, Validation Loss: 0.984910249710083, Validation MAE: 0.7948828339576721\n",
      "Epoch: 35, Training Loss: 0.9610456228256226, Validation Loss: 0.9542645812034607, Validation MAE: 0.7842593789100647\n",
      "Epoch: 36, Training Loss: 0.9622775912284851, Validation Loss: 0.9699478149414062, Validation MAE: 0.7889156341552734\n",
      "Epoch: 37, Training Loss: 0.9614077806472778, Validation Loss: 0.9408233761787415, Validation MAE: 0.7790077924728394\n",
      "Epoch: 38, Training Loss: 0.9616934657096863, Validation Loss: 0.9737505912780762, Validation MAE: 0.791214108467102\n",
      "Epoch: 39, Training Loss: 0.9617547392845154, Validation Loss: 0.9649494290351868, Validation MAE: 0.7877781391143799\n",
      "Epoch: 40, Training Loss: 0.961759090423584, Validation Loss: 1.007301926612854, Validation MAE: 0.7947787642478943\n",
      "Epoch: 41, Training Loss: 0.9630902409553528, Validation Loss: 0.9432197213172913, Validation MAE: 0.7801242470741272\n",
      "Epoch: 42, Training Loss: 0.9621683955192566, Validation Loss: 1.0321530103683472, Validation MAE: 0.8048490881919861\n",
      "Epoch: 43, Training Loss: 0.9610629081726074, Validation Loss: 0.9464860558509827, Validation MAE: 0.7816879749298096\n",
      "Epoch: 44, Training Loss: 0.9611818790435791, Validation Loss: 1.0119662284851074, Validation MAE: 0.7984561920166016\n",
      "Epoch: 45, Training Loss: 0.9614771604537964, Validation Loss: 0.9658505320549011, Validation MAE: 0.7880926132202148\n",
      "Epoch: 46, Training Loss: 0.9608426690101624, Validation Loss: 0.9795656800270081, Validation MAE: 0.7920434474945068\n",
      "Epoch: 47, Training Loss: 0.9609526991844177, Validation Loss: 0.9907959699630737, Validation MAE: 0.7947167158126831\n",
      "Epoch: 48, Training Loss: 0.9609750509262085, Validation Loss: 0.95257169008255, Validation MAE: 0.783609926700592\n",
      "Epoch: 49, Training Loss: 0.9606879353523254, Validation Loss: 1.0050139427185059, Validation MAE: 0.7969245910644531\n",
      "Epoch: 50, Training Loss: 0.9607571959495544, Validation Loss: 0.9623658061027527, Validation MAE: 0.786942183971405\n",
      "Epoch: 51, Training Loss: 0.9615126252174377, Validation Loss: 0.9953629970550537, Validation MAE: 0.79472416639328\n",
      "Epoch: 52, Training Loss: 0.9609230756759644, Validation Loss: 1.0009098052978516, Validation MAE: 0.7953862547874451\n",
      "Epoch: 53, Training Loss: 0.9605228304862976, Validation Loss: 0.9531236886978149, Validation MAE: 0.7837448120117188\n",
      "Epoch: 54, Training Loss: 0.9610025882720947, Validation Loss: 1.014797568321228, Validation MAE: 0.8026183843612671\n",
      "Epoch: 55, Training Loss: 0.9601331353187561, Validation Loss: 0.9625445604324341, Validation MAE: 0.7872270941734314\n",
      "Epoch: 56, Training Loss: 0.9602854251861572, Validation Loss: 1.0069060325622559, Validation MAE: 0.7985700964927673\n",
      "Epoch: 57, Training Loss: 0.961803674697876, Validation Loss: 0.9653444290161133, Validation MAE: 0.786989152431488\n",
      "Epoch: 58, Training Loss: 0.9604662656784058, Validation Loss: 0.9411594271659851, Validation MAE: 0.7786667346954346\n",
      "Epoch: 59, Training Loss: 0.9603587985038757, Validation Loss: 0.9755755066871643, Validation MAE: 0.7900984287261963\n",
      "Epoch: 60, Training Loss: 0.9601923227310181, Validation Loss: 0.993238627910614, Validation MAE: 0.7968440651893616\n",
      "Epoch: 61, Training Loss: 0.960044801235199, Validation Loss: 1.0244519710540771, Validation MAE: 0.7971804738044739\n",
      "Epoch: 62, Training Loss: 0.9605810046195984, Validation Loss: 1.023223638534546, Validation MAE: 0.8002707958221436\n",
      "Epoch: 63, Training Loss: 0.9602861404418945, Validation Loss: 0.9797232151031494, Validation MAE: 0.7911422252655029\n",
      "Epoch: 64, Training Loss: 0.9606083631515503, Validation Loss: 0.96347975730896, Validation MAE: 0.7867292761802673\n",
      "Epoch: 65, Training Loss: 0.9605504870414734, Validation Loss: 0.9542232155799866, Validation MAE: 0.7850790023803711\n",
      "Epoch: 66, Training Loss: 0.9597105383872986, Validation Loss: 0.9641584157943726, Validation MAE: 0.7875510454177856\n",
      "Epoch: 67, Training Loss: 0.9609066247940063, Validation Loss: 0.9607566595077515, Validation MAE: 0.7863654494285583\n",
      "Epoch: 68, Training Loss: 0.9604998230934143, Validation Loss: 0.9565902948379517, Validation MAE: 0.7844710946083069\n",
      "Epoch: 69, Training Loss: 0.9606549143791199, Validation Loss: 0.9362491369247437, Validation MAE: 0.7768223881721497\n",
      "Epoch: 70, Training Loss: 0.9597560167312622, Validation Loss: 0.9468289017677307, Validation MAE: 0.78128582239151\n",
      "Epoch: 71, Training Loss: 0.9603394269943237, Validation Loss: 0.9681761264801025, Validation MAE: 0.7896015048027039\n",
      "Epoch: 72, Training Loss: 0.9600743055343628, Validation Loss: 0.9518383145332336, Validation MAE: 0.7831299901008606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73, Training Loss: 0.95988529920578, Validation Loss: 0.9495419859886169, Validation MAE: 0.7828983664512634\n",
      "Epoch: 74, Training Loss: 0.9602516293525696, Validation Loss: 0.9552556872367859, Validation MAE: 0.7838786840438843\n",
      "Epoch: 75, Training Loss: 0.9604790210723877, Validation Loss: 0.9461085796356201, Validation MAE: 0.7811239957809448\n",
      "Epoch: 76, Training Loss: 0.9597371816635132, Validation Loss: 0.946458637714386, Validation MAE: 0.7808794379234314\n",
      "Epoch: 77, Training Loss: 0.9606422781944275, Validation Loss: 1.0070894956588745, Validation MAE: 0.796288251876831\n",
      "Epoch: 78, Training Loss: 0.9601771831512451, Validation Loss: 0.9917725324630737, Validation MAE: 0.7937639355659485\n",
      "Epoch: 79, Training Loss: 0.9600672125816345, Validation Loss: 0.9488013386726379, Validation MAE: 0.7811232209205627\n",
      "Epoch: 80, Training Loss: 0.959846019744873, Validation Loss: 0.9425500631332397, Validation MAE: 0.7791012525558472\n",
      "Epoch: 81, Training Loss: 0.9595492482185364, Validation Loss: 0.9381442070007324, Validation MAE: 0.7780718207359314\n",
      "Epoch: 82, Training Loss: 0.9600518345832825, Validation Loss: 0.9436333775520325, Validation MAE: 0.779977560043335\n",
      "Epoch: 83, Training Loss: 0.9593130946159363, Validation Loss: 0.9765242338180542, Validation MAE: 0.7889854311943054\n",
      "Epoch: 84, Training Loss: 0.9598836898803711, Validation Loss: 0.9605917930603027, Validation MAE: 0.7840962409973145\n",
      "Epoch: 85, Training Loss: 0.9594264030456543, Validation Loss: 0.9424243569374084, Validation MAE: 0.7791825532913208\n",
      "Epoch: 86, Training Loss: 0.9601120948791504, Validation Loss: 0.976739227771759, Validation MAE: 0.7891449928283691\n",
      "Epoch: 87, Training Loss: 0.9602677822113037, Validation Loss: 0.9459860920906067, Validation MAE: 0.7814590334892273\n",
      "Epoch: 88, Training Loss: 0.9593436121940613, Validation Loss: 0.9734750986099243, Validation MAE: 0.7898294925689697\n",
      "Epoch: 89, Training Loss: 0.9601195454597473, Validation Loss: 0.9430646896362305, Validation MAE: 0.7792492508888245\n",
      "Epoch: 90, Training Loss: 0.9593471884727478, Validation Loss: 0.9657192826271057, Validation MAE: 0.7881526947021484\n",
      "Epoch: 91, Training Loss: 0.959706723690033, Validation Loss: 0.9516093134880066, Validation MAE: 0.7835094332695007\n",
      "Epoch: 92, Training Loss: 0.9596803188323975, Validation Loss: 0.9473310708999634, Validation MAE: 0.7809220552444458\n",
      "Epoch: 93, Training Loss: 0.9596044421195984, Validation Loss: 0.9401175379753113, Validation MAE: 0.7783800363540649\n",
      "Epoch: 94, Training Loss: 0.9593983292579651, Validation Loss: 1.0142699480056763, Validation MAE: 0.7959234714508057\n",
      "Epoch: 95, Training Loss: 0.9595351815223694, Validation Loss: 0.954233705997467, Validation MAE: 0.7847867608070374\n",
      "Epoch: 96, Training Loss: 0.959314227104187, Validation Loss: 0.9738547801971436, Validation MAE: 0.7886226177215576\n",
      "Epoch: 97, Training Loss: 0.9595761895179749, Validation Loss: 0.9380995631217957, Validation MAE: 0.7774714231491089\n",
      "Epoch: 98, Training Loss: 0.9597076773643494, Validation Loss: 0.9333362579345703, Validation MAE: 0.775655210018158\n",
      "Epoch: 99, Training Loss: 0.9596076607704163, Validation Loss: 0.9466473460197449, Validation MAE: 0.7812808752059937\n",
      "Epoch: 100, Training Loss: 0.9594987034797668, Validation Loss: 0.9484581351280212, Validation MAE: 0.781936764717102\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a mask for validation data (we'll assume 15% of the data for validation)\n",
    "num_val_nodes = int(num_nodes * 0.15)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask[num_train_nodes:num_train_nodes+num_val_nodes] = True\n",
    "\n",
    "# Add the validation mask to the data object\n",
    "data['val_mask'] = val_mask\n",
    "\n",
    "# Define the model, optimizer, and loss function as before\n",
    "\n",
    "# Start training and validation\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_maes = []\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    out = model(data)  # Perform forward pass\n",
    "    \n",
    "    # Compute the loss on the training data\n",
    "    train_loss = loss_func(out[data['train_mask']], data['y'][data['train_mask']])\n",
    "    train_loss.backward()  # Perform backward pass\n",
    "    optimizer.step()  # Update model weights\n",
    "    train_losses.append(train_loss.item())  # Store the training loss from this epoch\n",
    "\n",
    "    # Compute the loss on the validation data, without affecting gradients\n",
    "    with torch.no_grad():\n",
    "        val_out = out[data['val_mask']]\n",
    "        val_loss = loss_func(val_out, data['y'][data['val_mask']])\n",
    "        val_losses.append(val_loss.item())  # Store the validation loss from this epoch\n",
    "\n",
    "        # Compute the mean absolute error on the validation data\n",
    "        val_mae = F.l1_loss(val_out, data['y'][data['val_mask']])\n",
    "        val_maes.append(val_mae.item())  # Store the validation MAE from this epoch\n",
    "\n",
    "    print(f'Epoch: {epoch+1}, Training Loss: {train_loss.item()}, Validation Loss: {val_loss.item()}, Validation MAE: {val_mae.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d30842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
